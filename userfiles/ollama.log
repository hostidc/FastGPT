nohup: ignoring input
Couldn't find '/home/codespace/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILMIHC9kfGvCBDFlhVZ3MXplMqpg+FiUznoDPGs/7XEn

2024/11/25 07:02:30 routes.go:1197: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:2562047h47m16.854775807s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:3 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/tmp/ollamamodels OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:5 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2024-11-25T07:02:30.510Z level=INFO source=images.go:753 msg="total blobs: 0"
time=2024-11-25T07:02:30.510Z level=INFO source=images.go:760 msg="total unused blobs removed: 0"
time=2024-11-25T07:02:30.510Z level=INFO source=routes.go:1248 msg="Listening on [::]:11434 (version 0.4.4)"
time=2024-11-25T07:02:30.510Z level=INFO source=common.go:135 msg="extracting embedded files" dir=/tmp/ollama1403321598/runners
time=2024-11-25T07:02:30.691Z level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners="[cuda_v12 rocm cpu cpu_avx cpu_avx2 cuda_v11]"
time=2024-11-25T07:02:30.691Z level=INFO source=gpu.go:221 msg="looking for compatible GPUs"
time=2024-11-25T07:02:30.708Z level=INFO source=gpu.go:386 msg="no compatible GPUs were discovered"
time=2024-11-25T07:02:30.708Z level=INFO source=types.go:123 msg="inference compute" id=0 library=cpu variant=avx2 compute="" driver=0.0 name="" total="15.6 GiB" available="13.6 GiB"
[GIN] 2024/11/25 - 07:02:40 | 200 |      46.216µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/25 - 07:02:40 | 200 |     221.233µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2024/11/25 - 07:02:47 | 200 |      21.109µs |       127.0.0.1 | HEAD     "/"
time=2024-11-25T07:02:49.447Z level=INFO source=download.go:175 msg="downloading 5ee4f07cdb9b in 16 120 MB part(s)"
time=2024-11-25T07:03:14.792Z level=INFO source=download.go:175 msg="downloading 66b9ea09bd5b in 1 68 B part(s)"
time=2024-11-25T07:03:17.200Z level=INFO source=download.go:175 msg="downloading eb4402837c78 in 1 1.5 KB part(s)"
time=2024-11-25T07:03:19.536Z level=INFO source=download.go:175 msg="downloading b5c0e5cf74cf in 1 7.4 KB part(s)"
time=2024-11-25T07:03:22.655Z level=INFO source=download.go:175 msg="downloading 161ddde4c9cd in 1 487 B part(s)"
[GIN] 2024/11/25 - 07:03:25 | 200 | 38.134218845s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/25 - 07:03:31 | 200 |      22.382µs |       127.0.0.1 | HEAD     "/"
time=2024-11-25T07:03:34.352Z level=INFO source=download.go:175 msg="downloading dde5aa3fc5ff in 16 126 MB part(s)"
time=2024-11-25T07:03:56.650Z level=INFO source=download.go:175 msg="downloading 966de95ca8a6 in 1 1.4 KB part(s)"
time=2024-11-25T07:03:59.633Z level=INFO source=download.go:175 msg="downloading fcc5a6bec9da in 1 7.7 KB part(s)"
time=2024-11-25T07:04:03.601Z level=INFO source=download.go:175 msg="downloading a70ff7e570d9 in 1 6.0 KB part(s)"
time=2024-11-25T07:04:06.603Z level=INFO source=download.go:175 msg="downloading 56bb8bd477a5 in 1 96 B part(s)"
time=2024-11-25T07:04:08.975Z level=INFO source=download.go:175 msg="downloading 34bb5ab01051 in 1 561 B part(s)"
[GIN] 2024/11/25 - 07:04:11 | 200 | 40.119361599s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/25 - 07:04:16 | 200 |      24.816µs |       127.0.0.1 | HEAD     "/"
time=2024-11-25T07:04:19.001Z level=INFO source=download.go:175 msg="downloading 970aa74c0a90 in 3 100 MB part(s)"
time=2024-11-25T07:04:23.388Z level=INFO source=download.go:175 msg="downloading c71d239df917 in 1 11 KB part(s)"
time=2024-11-25T07:04:25.741Z level=INFO source=download.go:175 msg="downloading ce4a164fc046 in 1 17 B part(s)"
time=2024-11-25T07:04:28.118Z level=INFO source=download.go:175 msg="downloading 31df23ea7daa in 1 420 B part(s)"
[GIN] 2024/11/25 - 07:04:30 | 200 | 14.037896782s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/25 - 07:04:36 | 200 |      20.167µs |       127.0.0.1 | HEAD     "/"
time=2024-11-25T07:04:39.034Z level=INFO source=download.go:175 msg="downloading 5a2017d7552f in 3 100 MB part(s)"
time=2024-11-25T07:04:44.387Z level=INFO source=download.go:175 msg="downloading 66853702653d in 1 266 B part(s)"
[GIN] 2024/11/25 - 07:04:46 | 200 |  9.578242597s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/25 - 07:04:55 | 200 |      27.902µs |       127.0.0.1 | HEAD     "/"
time=2024-11-25T07:04:58.645Z level=INFO source=download.go:175 msg="downloading 0d655da2f0b0 in 2 100 MB part(s)"
time=2024-11-25T07:05:05.209Z level=INFO source=download.go:175 msg="downloading 68693db5eb3e in 1 28 B part(s)"
time=2024-11-25T07:05:07.560Z level=INFO source=download.go:175 msg="downloading 92a6f4b0a39d in 1 335 B part(s)"
[GIN] 2024/11/25 - 07:05:08 | 200 | 13.195823088s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/25 - 07:06:02 | 200 |      19.507µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/25 - 07:06:02 | 200 |     681.893µs |       127.0.0.1 | GET      "/api/tags"
time=2024-11-25T07:19:45.183Z level=INFO source=server.go:105 msg="system memory" total="15.6 GiB" free="12.6 GiB" free_swap="0 B"
time=2024-11-25T07:19:45.184Z level=INFO source=memory.go:343 msg="offload to cpu" layers.requested=-1 layers.model=25 layers.offload=0 layers.split="" memory.available="[12.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="409.4 MiB" memory.required.partial="0 B" memory.required.kv="60.0 MiB" memory.required.allocations="[409.4 MiB]" memory.weights.total="223.2 MiB" memory.weights.repeating="206.3 MiB" memory.weights.nonrepeating="16.9 MiB" memory.graph.full="160.0 MiB" memory.graph.partial="160.0 MiB"
time=2024-11-25T07:19:45.184Z level=INFO source=server.go:383 msg="starting llama server" cmd="/tmp/ollama1403321598/runners/cpu_avx2/ollama_llama_server --model /tmp/ollamamodels/blobs/sha256-0d655da2f0b08a1210068e234792da4dfcb5cd2896dfd57a813f52ccc9d0ab95 --ctx-size 10240 --batch-size 512 --threads 2 --no-mmap --parallel 5 --port 41617"
time=2024-11-25T07:19:45.185Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-11-25T07:19:45.185Z level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-11-25T07:19:45.185Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-11-25T07:19:45.215Z level=INFO source=runner.go:916 msg="starting go runner"
time=2024-11-25T07:19:45.215Z level=INFO source=runner.go:917 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=2
time=2024-11-25T07:19:45.215Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:41617"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /tmp/ollamamodels/blobs/sha256-0d655da2f0b08a1210068e234792da4dfcb5cd2896dfd57a813f52ccc9d0ab95 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = m3e-large
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 2
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  12:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,21128]   = ["[PAD]", "[unused1]", "[unused2]", "...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,21128]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,21128]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  18:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  22:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q4_0:  144 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.0769 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 21128
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 1024
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 4096
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = -1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 335M
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 324.47 M
llm_load_print_meta: model size       = 181.16 MiB (4.68 BPW) 
llm_load_print_meta: general.name     = m3e-large
llm_load_print_meta: BOS token        = 0 '[PAD]'
llm_load_print_meta: EOS token        = 2 '[unused2]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 2 '[unused2]'
llm_load_print_meta: max token length = 48
llm_load_tensors: ggml ctx size =    0.16 MiB
llm_load_tensors:        CPU buffer size =   181.16 MiB
time=2024-11-25T07:19:45.437Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llama_new_context_with_model: n_ctx      = 10240
llama_new_context_with_model: n_batch    = 2560
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   960.00 MiB
llama_new_context_with_model: KV self size  =  960.00 MiB, K (f16):  480.00 MiB, V (f16):  480.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.02 MiB
llama_new_context_with_model:        CPU compute buffer size =    25.01 MiB
llama_new_context_with_model: graph nodes  = 850
llama_new_context_with_model: graph splits = 1
time=2024-11-25T07:19:46.190Z level=INFO source=server.go:601 msg="llama runner started in 1.01 seconds"
llama_get_logits_ith: invalid logits id 11, reason: no logits
SIGSEGV: segmentation violation
PC=0x63f04dc8da58 m=4 sigcode=1 addr=0x0
signal arrived during cgo execution

goroutine 23 gp=0xc000082fc0 m=4 mp=0xc000055808 [syscall]:
runtime.cgocall(0x63f04db09e40, 0xc00005ec60)
	runtime/cgocall.go:157 +0x4b fp=0xc00005ec38 sp=0xc00005ec00 pc=0x63f04d88b30b
github.com/ollama/ollama/llama._Cfunc_gpt_sampler_csample(0x7d261c000fc0, 0x7d2618006450, 0xb)
	_cgo_gotypes.go:454 +0x4f fp=0xc00005ec60 sp=0xc00005ec38 pc=0x63f04d9883ef
main.(*Server).processBatch.(*SamplingContext).Sample.func4(0x63f04e100080?, 0x0?, 0xb)
	github.com/ollama/ollama/llama/llama.go:677 +0x86 fp=0xc00005ecb0 sp=0xc00005ec60 pc=0x63f04db05bc6
github.com/ollama/ollama/llama.(*SamplingContext).Sample(...)
	github.com/ollama/ollama/llama/llama.go:677
main.(*Server).processBatch(0xc0000cc120, 0xc0000ca150, 0xc00004c710)
	github.com/ollama/ollama/llama/runner/runner.go:479 +0x6ab fp=0xc00005eed0 sp=0xc00005ecb0 pc=0x63f04db04e8b
main.(*Server).run(0xc0000cc120, {0x63f04de48dc0, 0xc0000980a0})
	github.com/ollama/ollama/llama/runner/runner.go:334 +0x1e5 fp=0xc00005efb8 sp=0xc00005eed0 pc=0x63f04db044a5
main.main.gowrap2()
	github.com/ollama/ollama/llama/runner/runner.go:955 +0x28 fp=0xc00005efe0 sp=0xc00005efb8 pc=0x63f04db09048
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00005efe8 sp=0xc00005efe0 pc=0x63f04d8f3d21
created by main.main in goroutine 1
	github.com/ollama/ollama/llama/runner/runner.go:955 +0xc52

goroutine 1 gp=0xc0000061c0 m=nil [IO wait]:
runtime.gopark(0xc00003e508?, 0x0?, 0xc0?, 0x61?, 0xc0000358b8?)
	runtime/proc.go:402 +0xce fp=0xc000035880 sp=0xc000035860 pc=0x63f04d8c1f4e
runtime.netpollblock(0xc000035918?, 0x4d88aa66?, 0xf0?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0000358b8 sp=0xc000035880 pc=0x63f04d8ba197
internal/poll.runtime_pollWait(0x7d26258d2770, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0000358d8 sp=0xc0000358b8 pc=0x63f04d8ee9e5
internal/poll.(*pollDesc).wait(0x3?, 0x3fe?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000035900 sp=0xc0000358d8 pc=0x63f04d93e907
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc0000fe080)
	internal/poll/fd_unix.go:611 +0x2ac fp=0xc0000359a8 sp=0xc000035900 pc=0x63f04d93fdcc
net.(*netFD).accept(0xc0000fe080)
	net/fd_unix.go:172 +0x29 fp=0xc000035a60 sp=0xc0000359a8 pc=0x63f04d9ae949
net.(*TCPListener).accept(0xc00008e1e0)
	net/tcpsock_posix.go:159 +0x1e fp=0xc000035a88 sp=0xc000035a60 pc=0x63f04d9bf67e
net.(*TCPListener).Accept(0xc00008e1e0)
	net/tcpsock.go:327 +0x30 fp=0xc000035ab8 sp=0xc000035a88 pc=0x63f04d9be9d0
net/http.(*onceCloseListener).Accept(0xc0000cc1b0?)
	<autogenerated>:1 +0x24 fp=0xc000035ad0 sp=0xc000035ab8 pc=0x63f04dae5be4
net/http.(*Server).Serve(0xc000118000, {0x63f04de48780, 0xc00008e1e0})
	net/http/server.go:3260 +0x33e fp=0xc000035c00 sp=0xc000035ad0 pc=0x63f04dadc9fe
main.main()
	github.com/ollama/ollama/llama/runner/runner.go:975 +0xfec fp=0xc000035f50 sp=0xc000035c00 pc=0x63f04db08dcc
runtime.main()
	runtime/proc.go:271 +0x29d fp=0xc000035fe0 sp=0xc000035f50 pc=0x63f04d8c1b1d
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000035fe8 sp=0xc000035fe0 pc=0x63f04d8f3d21

goroutine 18 gp=0xc000082380 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00004a7a8 sp=0xc00004a788 pc=0x63f04d8c1f4e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.forcegchelper()
	runtime/proc.go:326 +0xb8 fp=0xc00004a7e0 sp=0xc00004a7a8 pc=0x63f04d8c1dd8
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004a7e8 sp=0xc00004a7e0 pc=0x63f04d8f3d21
created by runtime.init.6 in goroutine 1
	runtime/proc.go:314 +0x1a

goroutine 19 gp=0xc000082540 m=nil [GC sweep wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00004af80 sp=0xc00004af60 pc=0x63f04d8c1f4e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.bgsweep(0xc000090000)
	runtime/mgcsweep.go:278 +0x94 fp=0xc00004afc8 sp=0xc00004af80 pc=0x63f04d8aca94
runtime.gcenable.gowrap1()
	runtime/mgc.go:203 +0x25 fp=0xc00004afe0 sp=0xc00004afc8 pc=0x63f04d8a15c5
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004afe8 sp=0xc00004afe0 pc=0x63f04d8f3d21
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:203 +0x66

goroutine 20 gp=0xc000082700 m=nil [GC scavenge wait]:
runtime.gopark(0xc000090000?, 0x63f04dd47390?, 0x1?, 0x0?, 0xc000082700?)
	runtime/proc.go:402 +0xce fp=0xc00004b778 sp=0xc00004b758 pc=0x63f04d8c1f4e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.(*scavengerState).park(0x63f04e0174e0)
	runtime/mgcscavenge.go:425 +0x49 fp=0xc00004b7a8 sp=0xc00004b778 pc=0x63f04d8aa489
runtime.bgscavenge(0xc000090000)
	runtime/mgcscavenge.go:653 +0x3c fp=0xc00004b7c8 sp=0xc00004b7a8 pc=0x63f04d8aaa1c
runtime.gcenable.gowrap2()
	runtime/mgc.go:204 +0x25 fp=0xc00004b7e0 sp=0xc00004b7c8 pc=0x63f04d8a1565
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004b7e8 sp=0xc00004b7e0 pc=0x63f04d8f3d21
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0xa5

goroutine 21 gp=0xc000082c40 m=nil [finalizer wait]:
runtime.gopark(0xc00004e648?, 0x63f04d894ec5?, 0xa8?, 0x1?, 0xc0000061c0?)
	runtime/proc.go:402 +0xce fp=0xc00004e620 sp=0xc00004e600 pc=0x63f04d8c1f4e
runtime.runfinq()
	runtime/mfinal.go:194 +0x107 fp=0xc00004e7e0 sp=0xc00004e620 pc=0x63f04d8a0607
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004e7e8 sp=0xc00004e7e0 pc=0x63f04d8f3d21
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:164 +0x3d

goroutine 24 gp=0xc000083500 m=nil [select]:
runtime.gopark(0xc00013fa28?, 0x2?, 0x30?, 0x21?, 0xc00013f7ec?)
	runtime/proc.go:402 +0xce fp=0xc00013f660 sp=0xc00013f640 pc=0x63f04d8c1f4e
runtime.selectgo(0xc00013fa28, 0xc00013f7e8, 0xc?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc00013f780 sp=0xc00013f660 pc=0x63f04d8d3325
main.(*Server).completion(0xc0000cc120, {0x63f04de48930, 0xc0000b67e0}, 0xc0000d07e0)
	github.com/ollama/ollama/llama/runner/runner.go:690 +0xa86 fp=0xc00013fab8 sp=0xc00013f780 pc=0x63f04db06886
main.(*Server).completion-fm({0x63f04de48930?, 0xc0000b67e0?}, 0x63f04dae0d2d?)
	<autogenerated>:1 +0x36 fp=0xc00013fae8 sp=0xc00013fab8 pc=0x63f04db09836
net/http.HandlerFunc.ServeHTTP(0xc0000a8ea0?, {0x63f04de48930?, 0xc0000b67e0?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc00013fb10 sp=0xc00013fae8 pc=0x63f04dad97c9
net/http.(*ServeMux).ServeHTTP(0x63f04d894ec5?, {0x63f04de48930, 0xc0000b67e0}, 0xc0000d07e0)
	net/http/server.go:2688 +0x1ad fp=0xc00013fb60 sp=0xc00013fb10 pc=0x63f04dadb64d
net/http.serverHandler.ServeHTTP({0x63f04de47c80?}, {0x63f04de48930?, 0xc0000b67e0?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc00013fb90 sp=0xc00013fb60 pc=0x63f04dadc66e
net/http.(*conn).serve(0xc0000cc1b0, {0x63f04de48d88, 0xc0000a6de0})
	net/http/server.go:2044 +0x5e8 fp=0xc00013ffb8 sp=0xc00013fb90 pc=0x63f04dad8408
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc00013ffe0 sp=0xc00013ffb8 pc=0x63f04dadcde8
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00013ffe8 sp=0xc00013ffe0 pc=0x63f04d8f3d21
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 27 gp=0xc0001a4000 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0xbd?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc00004bda8 sp=0xc00004bd88 pc=0x63f04d8c1f4e
runtime.netpollblock(0x63f04d928498?, 0x4d88aa66?, 0xf0?)
	runtime/netpoll.go:573 +0xf7 fp=0xc00004bde0 sp=0xc00004bda8 pc=0x63f04d8ba197
internal/poll.runtime_pollWait(0x7d26258d2678, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc00004be00 sp=0xc00004bde0 pc=0x63f04d8ee9e5
internal/poll.(*pollDesc).wait(0xc0000fe100?, 0xc0000a6ee1?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00004be28 sp=0xc00004be00 pc=0x63f04d93e907
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc0000fe100, {0xc0000a6ee1, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc00004bec0 sp=0xc00004be28 pc=0x63f04d93f45a
net.(*netFD).Read(0xc0000fe100, {0xc0000a6ee1?, 0xc00004bf48?, 0x63f04d8f0610?})
	net/fd_posix.go:55 +0x25 fp=0xc00004bf08 sp=0xc00004bec0 pc=0x63f04d9ad845
net.(*conn).Read(0xc0000a4088, {0xc0000a6ee1?, 0x0?, 0x63f04e100080?})
	net/net.go:185 +0x45 fp=0xc00004bf50 sp=0xc00004bf08 pc=0x63f04d9b7b05
net.(*TCPConn).Read(0x63f04dfd8810?, {0xc0000a6ee1?, 0x7ffe66f74588?, 0x5f?})
	<autogenerated>:1 +0x25 fp=0xc00004bf80 sp=0xc00004bf50 pc=0x63f04d9c34e5
net/http.(*connReader).backgroundRead(0xc0000a6ed0)
	net/http/server.go:681 +0x37 fp=0xc00004bfc8 sp=0xc00004bf80 pc=0x63f04dad2377
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc00004bfe0 sp=0xc00004bfc8 pc=0x63f04dad22a5
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004bfe8 sp=0xc00004bfe0 pc=0x63f04d8f3d21
created by net/http.(*connReader).startBackgroundRead in goroutine 24
	net/http/server.go:677 +0xba

rax    0x0
rbx    0x0
rcx    0x5288
rdx    0x7d261c00fd10
rdi    0x7d261c00fd10
rsi    0x5288
rbp    0x7d2618006450
rsp    0x7d2626107d80
r8     0x5288
r9     0x0
r10    0x7d261c021000
r11    0x206
r12    0xb
r13    0x7d261c001090
r14    0x7d261c000fc0
r15    0x0
rip    0x63f04dc8da58
rflags 0x10206
cs     0x33
fs     0x0
gs     0x0
[GIN] 2024/11/25 - 07:19:46 | 500 |  1.193032407s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:19:46 | 400 |    3.480023ms |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:19:46 | 400 |    6.185643ms |      172.18.0.7 | POST     "/api/chat"
time=2024-11-25T07:19:46.440Z level=INFO source=server.go:105 msg="system memory" total="15.6 GiB" free="12.7 GiB" free_swap="0 B"
time=2024-11-25T07:19:46.441Z level=INFO source=memory.go:343 msg="offload to cpu" layers.requested=-1 layers.model=37 layers.offload=0 layers.split="" memory.available="[12.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.5 GiB" memory.required.partial="0 B" memory.required.kv="360.0 MiB" memory.required.allocations="[2.5 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.7 GiB" memory.weights.nonrepeating="243.4 MiB" memory.graph.full="348.0 MiB" memory.graph.partial="544.2 MiB"
time=2024-11-25T07:19:46.441Z level=INFO source=server.go:383 msg="starting llama server" cmd="/tmp/ollama1403321598/runners/cpu_avx2/ollama_llama_server --model /tmp/ollamamodels/blobs/sha256-5ee4f07cdb9beadbbb293e85803c569b01bd37ed059d2715faa7bb405f31caa6 --ctx-size 10240 --batch-size 512 --threads 2 --no-mmap --parallel 5 --port 43287"
time=2024-11-25T07:19:46.441Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2024-11-25T07:19:46.441Z level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-11-25T07:19:46.442Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-11-25T07:19:46.445Z level=INFO source=runner.go:916 msg="starting go runner"
time=2024-11-25T07:19:46.446Z level=INFO source=runner.go:917 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=2
time=2024-11-25T07:19:46.446Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:43287"
llama_model_loader: loaded meta data with 35 key-value pairs and 434 tensors from /tmp/ollamamodels/blobs/sha256-5ee4f07cdb9beadbbb293e85803c569b01bd37ed059d2715faa7bb405f31caa6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = other
llama_model_loader: - kv   7:                       general.license.name str              = qwen-research
llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-3...
llama_model_loader: - kv   9:                   general.base_model.count u32              = 1
llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 3B
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-3B
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  14:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  15:                          qwen2.block_count u32              = 36
llama_model_loader: - kv  16:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  17:                     qwen2.embedding_length u32              = 2048
llama_model_loader: - kv  18:                  qwen2.feed_forward_length u32              = 11008
llama_model_loader: - kv  19:                 qwen2.attention.head_count u32              = 16
llama_model_loader: - kv  20:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  21:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  23:                          general.file_type u32              = 15
llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  34:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  181 tensors
llama_model_loader: - type q4_K:  216 tensors
llama_model_loader: - type q6_K:   37 tensors
time=2024-11-25T07:19:46.693Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 151936
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 36
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 2
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.09 B
llm_load_print_meta: model size       = 1.79 GiB (4.99 BPW) 
llm_load_print_meta: general.name     = Qwen2.5 3B Instruct
llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOS token        = 151645 '<|im_end|>'
llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: EOT token        = 151645 '<|im_end|>'
llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151645 '<|im_end|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: ggml ctx size =    0.19 MiB
llm_load_tensors:        CPU buffer size =  2078.25 MiB
llama_new_context_with_model: n_ctx      = 10240
llama_new_context_with_model: n_batch    = 2560
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   360.00 MiB
llama_new_context_with_model: KV self size  =  360.00 MiB, K (f16):  180.00 MiB, V (f16):  180.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     2.94 MiB
llama_new_context_with_model:        CPU compute buffer size =   356.01 MiB
llama_new_context_with_model: graph nodes  = 1266
llama_new_context_with_model: graph splits = 1
time=2024-11-25T07:19:51.460Z level=INFO source=server.go:601 msg="llama runner started in 5.02 seconds"
llama_model_loader: loaded meta data with 35 key-value pairs and 434 tensors from /tmp/ollamamodels/blobs/sha256-5ee4f07cdb9beadbbb293e85803c569b01bd37ed059d2715faa7bb405f31caa6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = other
llama_model_loader: - kv   7:                       general.license.name str              = qwen-research
llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-3...
llama_model_loader: - kv   9:                   general.base_model.count u32              = 1
llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 3B
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-3B
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  14:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  15:                          qwen2.block_count u32              = 36
llama_model_loader: - kv  16:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  17:                     qwen2.embedding_length u32              = 2048
llama_model_loader: - kv  18:                  qwen2.feed_forward_length u32              = 11008
llama_model_loader: - kv  19:                 qwen2.attention.head_count u32              = 16
llama_model_loader: - kv  20:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  21:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  23:                          general.file_type u32              = 15
llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  34:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  181 tensors
llama_model_loader: - type q4_K:  216 tensors
llama_model_loader: - type q6_K:   37 tensors
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 151936
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 3.09 B
llm_load_print_meta: model size       = 1.79 GiB (4.99 BPW) 
llm_load_print_meta: general.name     = Qwen2.5 3B Instruct
llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOS token        = 151645 '<|im_end|>'
llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: EOT token        = 151645 '<|im_end|>'
llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151645 '<|im_end|>'
llm_load_print_meta: max token length = 256
llama_model_load: vocab only - skipping tensors
[GIN] 2024/11/25 - 07:19:56 | 200 | 10.518211942s |      172.18.0.7 | POST     "/api/chat"
time=2024-11-25T07:19:56.973Z level=INFO source=server.go:105 msg="system memory" total="15.6 GiB" free="10.1 GiB" free_swap="0 B"
time=2024-11-25T07:19:56.973Z level=INFO source=memory.go:343 msg="offload to cpu" layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[10.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.6 GiB" memory.required.partial="0 B" memory.required.kv="1.1 GiB" memory.required.allocations="[3.6 GiB]" memory.weights.total="2.7 GiB" memory.weights.repeating="2.4 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="524.0 MiB" memory.graph.partial="570.7 MiB"
time=2024-11-25T07:19:56.974Z level=INFO source=server.go:383 msg="starting llama server" cmd="/tmp/ollama1403321598/runners/cpu_avx2/ollama_llama_server --model /tmp/ollamamodels/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 10240 --batch-size 512 --threads 2 --no-mmap --parallel 5 --port 34859"
time=2024-11-25T07:19:56.974Z level=INFO source=sched.go:449 msg="loaded runners" count=3
time=2024-11-25T07:19:56.974Z level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-11-25T07:19:56.974Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-11-25T07:19:56.978Z level=INFO source=runner.go:916 msg="starting go runner"
time=2024-11-25T07:19:56.979Z level=INFO source=runner.go:917 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=2
time=2024-11-25T07:19:56.979Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:34859"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /tmp/ollamamodels/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
time=2024-11-25T07:19:57.225Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 3072
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 24
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 3
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.21 B
llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) 
llm_load_print_meta: general.name     = Llama 3.2 3B Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2226.59 MiB
llama_new_context_with_model: n_ctx      = 10240
llama_new_context_with_model: n_batch    = 2560
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =  1120.00 MiB
llama_new_context_with_model: KV self size  = 1120.00 MiB, K (f16):  560.00 MiB, V (f16):  560.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     2.50 MiB
llama_new_context_with_model:        CPU compute buffer size =   524.01 MiB
llama_new_context_with_model: graph nodes  = 902
llama_new_context_with_model: graph splits = 1
time=2024-11-25T07:20:02.494Z level=INFO source=server.go:601 msg="llama runner started in 5.52 seconds"
[GIN] 2024/11/25 - 07:20:05 | 200 |  8.182840517s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:20:05 | 404 |     352.911µs |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:20:05 | 404 |     326.459µs |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:20:05 | 404 |      52.137µs |      172.18.0.7 | POST     "/api/chat"
time=2024-11-25T07:25:26.948Z level=INFO source=server.go:105 msg="system memory" total="15.6 GiB" free="6.7 GiB" free_swap="0 B"
time=2024-11-25T07:25:26.949Z level=INFO source=memory.go:343 msg="offload to cpu" layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[6.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2024-11-25T07:25:26.949Z level=INFO source=server.go:383 msg="starting llama server" cmd="/tmp/ollama1403321598/runners/cpu_avx2/ollama_llama_server --model /tmp/ollamamodels/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 46517"
time=2024-11-25T07:25:26.950Z level=INFO source=sched.go:449 msg="loaded runners" count=3
time=2024-11-25T07:25:26.950Z level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-11-25T07:25:26.950Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-11-25T07:25:26.954Z level=INFO source=runner.go:916 msg="starting go runner"
time=2024-11-25T07:25:26.954Z level=INFO source=runner.go:917 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=2
time=2024-11-25T07:25:26.954Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:46517"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /tmp/ollamamodels/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: ggml ctx size =    0.05 MiB
llm_load_tensors:        CPU buffer size =   260.86 MiB
time=2024-11-25T07:25:27.201Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llama_new_context_with_model: n_ctx      = 8192
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 1000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 1
time=2024-11-25T07:25:27.704Z level=INFO source=server.go:601 msg="llama runner started in 0.75 seconds"
[GIN] 2024/11/25 - 07:25:29 | 200 |  2.710781068s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:31 | 200 |  4.996965936s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:33 | 200 |  6.322489985s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:35 | 200 |  5.028492507s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:37 | 200 |  5.401772583s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:39 | 200 |  6.248593045s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:41 | 200 |  6.340088221s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:43 | 200 |  6.374695905s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:45 | 200 |  6.314517519s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:47 | 200 |  6.027938129s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:49 | 200 |  6.020687919s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:51 | 200 |  5.506533646s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:53 | 200 |  5.288325631s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:54 | 200 |  5.046561667s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:56 | 200 |  5.112929989s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:25:58 | 200 |  5.190673055s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:00 | 200 |  5.516598235s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:02 | 200 |  5.987331139s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:04 | 200 |  6.123506731s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:06 | 200 |  5.734676777s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:08 | 200 |  5.935342357s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:10 | 200 |  5.949168919s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:12 | 200 |  6.207451691s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:14 | 200 |  6.078004325s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:16 | 200 |  6.168220678s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:18 | 200 |  6.366638124s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:20 | 200 |   6.30890468s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:23 | 200 |  6.459291543s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:24 | 200 |  5.919985463s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:26 | 200 |  5.522646578s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:28 | 200 |  5.190267385s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:30 | 200 |  5.557542784s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:32 | 200 |  6.034150007s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:34 | 200 |   5.97684111s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:35 | 200 |  4.884235109s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:36 | 200 |  3.453255304s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:40 | 200 |  6.020292606s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:42 | 200 |  6.782728372s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:44 | 200 |  8.695548834s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:46 | 200 |  5.989026784s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:47 | 200 |  5.828827378s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:48 | 200 |  4.128359472s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:49 | 200 |  3.537533438s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:50 | 200 |  2.393151565s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:51 | 200 |  2.932232945s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:53 | 200 |  4.089304426s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:55 | 200 |   5.37202678s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:56 | 200 |   5.10252554s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:57 | 200 |  3.926004972s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:58 | 200 |  2.493660261s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:58 | 200 |   1.44373584s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:58 | 200 |  686.536574ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:26:59 | 200 |  1.238435427s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:00 | 200 |  2.009113935s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:00 | 200 |  2.026276829s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:01 | 200 |   1.52579519s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:01 | 200 |  927.215512ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:01 | 200 |  904.395691ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:01 | 200 |  610.033906ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:02 | 200 |  618.436932ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:02 | 200 |  931.088506ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:02 | 200 |   1.18156259s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:03 | 200 |  1.105494624s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:04 | 200 |   1.42499573s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:04 | 200 |  1.608773106s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:04 | 200 |      24.836µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/25 - 07:27:04 | 200 |     121.126µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2024/11/25 - 07:27:05 | 200 |  2.295693446s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:05 | 200 |    1.8521868s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:27:08 | 200 |  3.587625936s |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:29:06 | 200 |   97.157945ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:29:14 | 200 |      25.267µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/25 - 07:29:14 | 200 |      44.473µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2024/11/25 - 07:29:26 | 200 |      25.197µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/25 - 07:29:26 | 200 |      36.358µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2024/11/25 - 07:31:05 | 200 |         1m58s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:33:54 | 200 |  133.530052ms |      172.18.0.7 | POST     "/api/embeddings"
[GIN] 2024/11/25 - 07:34:35 | 200 |         4m28s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:36:32 | 200 |         5m24s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:36:50 | 200 |         1m14s |      172.18.0.7 | POST     "/api/chat"
time=2024-11-25T07:39:14.762Z level=INFO source=server.go:105 msg="system memory" total="15.6 GiB" free="6.2 GiB" free_swap="0 B"
time=2024-11-25T07:39:14.763Z level=INFO source=memory.go:343 msg="offload to cpu" layers.requested=-1 layers.model=25 layers.offload=0 layers.split="" memory.available="[6.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="409.4 MiB" memory.required.partial="0 B" memory.required.kv="60.0 MiB" memory.required.allocations="[409.4 MiB]" memory.weights.total="223.2 MiB" memory.weights.repeating="206.3 MiB" memory.weights.nonrepeating="16.9 MiB" memory.graph.full="160.0 MiB" memory.graph.partial="160.0 MiB"
time=2024-11-25T07:39:14.763Z level=INFO source=server.go:383 msg="starting llama server" cmd="/tmp/ollama1403321598/runners/cpu_avx2/ollama_llama_server --model /tmp/ollamamodels/blobs/sha256-0d655da2f0b08a1210068e234792da4dfcb5cd2896dfd57a813f52ccc9d0ab95 --ctx-size 10240 --batch-size 512 --threads 2 --no-mmap --parallel 5 --port 44511"
time=2024-11-25T07:39:14.764Z level=INFO source=sched.go:449 msg="loaded runners" count=3
time=2024-11-25T07:39:14.764Z level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-11-25T07:39:14.764Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-11-25T07:39:14.769Z level=INFO source=runner.go:916 msg="starting go runner"
time=2024-11-25T07:39:14.770Z level=INFO source=runner.go:917 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=2
time=2024-11-25T07:39:14.770Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:44511"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /tmp/ollamamodels/blobs/sha256-0d655da2f0b08a1210068e234792da4dfcb5cd2896dfd57a813f52ccc9d0ab95 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = m3e-large
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 2
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  11:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  12:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,21128]   = ["[PAD]", "[unused1]", "[unused2]", "...
llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,21128]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,21128]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  18:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  22:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q4_0:  144 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.0769 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 21128
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 1024
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 4096
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = -1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 335M
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 324.47 M
llm_load_print_meta: model size       = 181.16 MiB (4.68 BPW) 
llm_load_print_meta: general.name     = m3e-large
llm_load_print_meta: BOS token        = 0 '[PAD]'
llm_load_print_meta: EOS token        = 2 '[unused2]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 2 '[unused2]'
llm_load_print_meta: max token length = 48
llm_load_tensors: ggml ctx size =    0.16 MiB
llm_load_tensors:        CPU buffer size =   181.16 MiB
time=2024-11-25T07:39:15.016Z level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llama_new_context_with_model: n_ctx      = 10240
llama_new_context_with_model: n_batch    = 2560
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   960.00 MiB
llama_new_context_with_model: KV self size  =  960.00 MiB, K (f16):  480.00 MiB, V (f16):  480.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.02 MiB
llama_new_context_with_model:        CPU compute buffer size =    25.01 MiB
llama_new_context_with_model: graph nodes  = 850
llama_new_context_with_model: graph splits = 1
time=2024-11-25T07:39:15.768Z level=INFO source=server.go:601 msg="llama runner started in 1.00 seconds"
llama_get_logits_ith: invalid logits id 11, reason: no logits
SIGSEGV: segmentation violation
PC=0x5e8946ed7a58 m=3 sigcode=1 addr=0x0
signal arrived during cgo execution

goroutine 20 gp=0xc00008aa80 m=3 mp=0xc000055008 [syscall]:
runtime.cgocall(0x5e8946d53e40, 0xc00005ec60)
	runtime/cgocall.go:157 +0x4b fp=0xc00005ec38 sp=0xc00005ec00 pc=0x5e8946ad530b
github.com/ollama/ollama/llama._Cfunc_gpt_sampler_csample(0x71ce90001120, 0x5e894903dcf0, 0xb)
	_cgo_gotypes.go:454 +0x4f fp=0xc00005ec60 sp=0xc00005ec38 pc=0x5e8946bd23ef
main.(*Server).processBatch.(*SamplingContext).Sample.func4(0x5e894734a080?, 0x0?, 0xb)
	github.com/ollama/ollama/llama/llama.go:677 +0x86 fp=0xc00005ecb0 sp=0xc00005ec60 pc=0x5e8946d4fbc6
github.com/ollama/ollama/llama.(*SamplingContext).Sample(...)
	github.com/ollama/ollama/llama/llama.go:677
main.(*Server).processBatch(0xc0000cc120, 0xc0001b2000, 0xc00005ef10)
	github.com/ollama/ollama/llama/runner/runner.go:479 +0x6ab fp=0xc00005eed0 sp=0xc00005ecb0 pc=0x5e8946d4ee8b
main.(*Server).run(0xc0000cc120, {0x5e8947092dc0, 0xc000110050})
	github.com/ollama/ollama/llama/runner/runner.go:334 +0x1e5 fp=0xc00005efb8 sp=0xc00005eed0 pc=0x5e8946d4e4a5
main.main.gowrap2()
	github.com/ollama/ollama/llama/runner/runner.go:955 +0x28 fp=0xc00005efe0 sp=0xc00005efb8 pc=0x5e8946d53048
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00005efe8 sp=0xc00005efe0 pc=0x5e8946b3dd21
created by main.main in goroutine 1
	github.com/ollama/ollama/llama/runner/runner.go:955 +0xc52

goroutine 1 gp=0xc0000061c0 m=nil [IO wait]:
runtime.gopark(0x1?, 0xc000035900?, 0x34?, 0xbd?, 0xc0000358e0?)
	runtime/proc.go:402 +0xce fp=0xc000035880 sp=0xc000035860 pc=0x5e8946b0bf4e
runtime.netpollblock(0x10?, 0x46ad4a66?, 0x89?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0000358b8 sp=0xc000035880 pc=0x5e8946b04197
internal/poll.runtime_pollWait(0x71ce9b7c9770, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0000358d8 sp=0xc0000358b8 pc=0x5e8946b389e5
internal/poll.(*pollDesc).wait(0x3?, 0x71ce99ec6328?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000035900 sp=0xc0000358d8 pc=0x5e8946b88907
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc00010a080)
	internal/poll/fd_unix.go:611 +0x2ac fp=0xc0000359a8 sp=0xc000035900 pc=0x5e8946b89dcc
net.(*netFD).accept(0xc00010a080)
	net/fd_unix.go:172 +0x29 fp=0xc000035a60 sp=0xc0000359a8 pc=0x5e8946bf8949
net.(*TCPListener).accept(0xc0000ce1c0)
	net/tcpsock_posix.go:159 +0x1e fp=0xc000035a88 sp=0xc000035a60 pc=0x5e8946c0967e
net.(*TCPListener).Accept(0xc0000ce1c0)
	net/tcpsock.go:327 +0x30 fp=0xc000035ab8 sp=0xc000035a88 pc=0x5e8946c089d0
net/http.(*onceCloseListener).Accept(0xc000188000?)
	<autogenerated>:1 +0x24 fp=0xc000035ad0 sp=0xc000035ab8 pc=0x5e8946d2fbe4
net/http.(*Server).Serve(0xc000126000, {0x5e8947092780, 0xc0000ce1c0})
	net/http/server.go:3260 +0x33e fp=0xc000035c00 sp=0xc000035ad0 pc=0x5e8946d269fe
main.main()
	github.com/ollama/ollama/llama/runner/runner.go:975 +0xfec fp=0xc000035f50 sp=0xc000035c00 pc=0x5e8946d52dcc
runtime.main()
	runtime/proc.go:271 +0x29d fp=0xc000035fe0 sp=0xc000035f50 pc=0x5e8946b0bb1d
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000035fe8 sp=0xc000035fe0 pc=0x5e8946b3dd21

goroutine 2 gp=0xc000006c40 m=nil [force gc (idle)]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00004efa8 sp=0xc00004ef88 pc=0x5e8946b0bf4e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.forcegchelper()
	runtime/proc.go:326 +0xb8 fp=0xc00004efe0 sp=0xc00004efa8 pc=0x5e8946b0bdd8
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004efe8 sp=0xc00004efe0 pc=0x5e8946b3dd21
created by runtime.init.6 in goroutine 1
	runtime/proc.go:314 +0x1a

goroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00004f780 sp=0xc00004f760 pc=0x5e8946b0bf4e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.bgsweep(0xc000028070)
	runtime/mgcsweep.go:278 +0x94 fp=0xc00004f7c8 sp=0xc00004f780 pc=0x5e8946af6a94
runtime.gcenable.gowrap1()
	runtime/mgc.go:203 +0x25 fp=0xc00004f7e0 sp=0xc00004f7c8 pc=0x5e8946aeb5c5
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004f7e8 sp=0xc00004f7e0 pc=0x5e8946b3dd21
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:203 +0x66

goroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:
runtime.gopark(0xc000028070?, 0x5e8946f91390?, 0x1?, 0x0?, 0xc000007340?)
	runtime/proc.go:402 +0xce fp=0xc00004ff78 sp=0xc00004ff58 pc=0x5e8946b0bf4e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.(*scavengerState).park(0x5e89472614e0)
	runtime/mgcscavenge.go:425 +0x49 fp=0xc00004ffa8 sp=0xc00004ff78 pc=0x5e8946af4489
runtime.bgscavenge(0xc000028070)
	runtime/mgcscavenge.go:653 +0x3c fp=0xc00004ffc8 sp=0xc00004ffa8 pc=0x5e8946af4a1c
runtime.gcenable.gowrap2()
	runtime/mgc.go:204 +0x25 fp=0xc00004ffe0 sp=0xc00004ffc8 pc=0x5e8946aeb565
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004ffe8 sp=0xc00004ffe0 pc=0x5e8946b3dd21
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0xa5

goroutine 18 gp=0xc00008a700 m=nil [finalizer wait]:
runtime.gopark(0xc00004e648?, 0x5e8946adeec5?, 0xa8?, 0x1?, 0xc0000061c0?)
	runtime/proc.go:402 +0xce fp=0xc00004e620 sp=0xc00004e600 pc=0x5e8946b0bf4e
runtime.runfinq()
	runtime/mfinal.go:194 +0x107 fp=0xc00004e7e0 sp=0xc00004e620 pc=0x5e8946aea607
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00004e7e8 sp=0xc00004e7e0 pc=0x5e8946b3dd21
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:164 +0x3d

goroutine 34 gp=0xc00018e000 m=nil [select]:
runtime.gopark(0xc00022ba28?, 0x2?, 0x10?, 0x1?, 0xc00022b7ec?)
	runtime/proc.go:402 +0xce fp=0xc00022b660 sp=0xc00022b640 pc=0x5e8946b0bf4e
runtime.selectgo(0xc00022ba28, 0xc00022b7e8, 0xc?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc00022b780 sp=0xc00022b660 pc=0x5e8946b1d325
main.(*Server).completion(0xc0000cc120, {0x5e8947092930, 0xc000016380}, 0xc000202480)
	github.com/ollama/ollama/llama/runner/runner.go:690 +0xa86 fp=0xc00022bab8 sp=0xc00022b780 pc=0x5e8946d50886
main.(*Server).completion-fm({0x5e8947092930?, 0xc000016380?}, 0x5e8946d2ad2d?)
	<autogenerated>:1 +0x36 fp=0xc00022bae8 sp=0xc00022bab8 pc=0x5e8946d53836
net/http.HandlerFunc.ServeHTTP(0xc0000a7110?, {0x5e8947092930?, 0xc000016380?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc00022bb10 sp=0xc00022bae8 pc=0x5e8946d237c9
net/http.(*ServeMux).ServeHTTP(0x5e8946adeec5?, {0x5e8947092930, 0xc000016380}, 0xc000202480)
	net/http/server.go:2688 +0x1ad fp=0xc00022bb60 sp=0xc00022bb10 pc=0x5e8946d2564d
net/http.serverHandler.ServeHTTP({0x5e8947091c80?}, {0x5e8947092930?, 0xc000016380?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc00022bb90 sp=0xc00022bb60 pc=0x5e8946d2666e
net/http.(*conn).serve(0xc000188000, {0x5e8947092d88, 0xc0000a4de0})
	net/http/server.go:2044 +0x5e8 fp=0xc00022bfb8 sp=0xc00022bb90 pc=0x5e8946d22408
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc00022bfe0 sp=0xc00022bfb8 pc=0x5e8946d26de8
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00022bfe8 sp=0xc00022bfe0 pc=0x5e8946b3dd21
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 8 gp=0xc000007c00 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0x5?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0000505a8 sp=0xc000050588 pc=0x5e8946b0bf4e
runtime.netpollblock(0x5e8946b72498?, 0x46ad4a66?, 0x89?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0000505e0 sp=0xc0000505a8 pc=0x5e8946b04197
internal/poll.runtime_pollWait(0x71ce9b7c9678, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc000050600 sp=0xc0000505e0 pc=0x5e8946b389e5
internal/poll.(*pollDesc).wait(0xc000182000?, 0xc0000a4e81?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000050628 sp=0xc000050600 pc=0x5e8946b88907
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000182000, {0xc0000a4e81, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0000506c0 sp=0xc000050628 pc=0x5e8946b8945a
net.(*netFD).Read(0xc000182000, {0xc0000a4e81?, 0xc000050748?, 0x5e8946b3a610?})
	net/fd_posix.go:55 +0x25 fp=0xc000050708 sp=0xc0000506c0 pc=0x5e8946bf7845
net.(*conn).Read(0xc000186000, {0xc0000a4e81?, 0x0?, 0x5e894734a080?})
	net/net.go:185 +0x45 fp=0xc000050750 sp=0xc000050708 pc=0x5e8946c01b05
net.(*TCPConn).Read(0x5e8947222810?, {0xc0000a4e81?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc000050780 sp=0xc000050750 pc=0x5e8946c0d4e5
net/http.(*connReader).backgroundRead(0xc0000a4e70)
	net/http/server.go:681 +0x37 fp=0xc0000507c8 sp=0xc000050780 pc=0x5e8946d1c377
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0000507e0 sp=0xc0000507c8 pc=0x5e8946d1c2a5
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000507e8 sp=0xc0000507e0 pc=0x5e8946b3dd21
created by net/http.(*connReader).startBackgroundRead in goroutine 34
	net/http/server.go:677 +0xba

rax    0x0
rbx    0x0
rcx    0x5288
rdx    0x71ce9000fe70
rdi    0x71ce9000fe70
rsi    0x5288
rbp    0x5e894903dcf0
rsp    0x71ce9801ad80
r8     0x5288
r9     0x0
r10    0x71ce90021000
r11    0x206
r12    0xb
r13    0x71ce900011f0
r14    0x71ce90001120
r15    0x0
rip    0x5e8946ed7a58
rflags 0x10206
cs     0x33
fs     0x0
gs     0x0
[GIN] 2024/11/25 - 07:39:16 | 500 |   1.33154995s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:39:16 | 400 |    4.388297ms |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:39:16 | 400 |     6.60901ms |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:46:27 | 200 |         7m56s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:29 | 200 |        10m57s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:29 | 200 |         9m56s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:31 | 200 |        11m15s |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:32 | 200 |  891.137834ms |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:32 | 404 |      88.215µs |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:32 | 404 |      65.732µs |      172.18.0.7 | POST     "/api/chat"
[GIN] 2024/11/25 - 07:50:32 | 404 |      55.333µs |      172.18.0.7 | POST     "/api/chat"
